import pandas as pd
from sklearn.cross_validation import train_test_split
from sklearn.svm import SVC
from sklearn import preprocessing
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import RFE, RFECV
from sklearn import metrics
import numpy as np
from sklearn.feature_selection import SelectKBest,f_classif
#np.random.seed(105)
kf=StratifiedKFold(n_splits=5)
## Reading the datasets of all the sessions................
DataSet_1=pd.read_csv('Exported_Dataset_Ses_1/All_DS_Ses_1.csv')
DataSet_2=pd.read_csv('Exported_Dataset_Ses_2/All_DS_Ses_2.csv')
DataSet_3=pd.read_csv('Exported_Dataset_Ses_3/All_DS_Ses_3.csv')
#...................the first consideration is putting all the data set's of all sessions in one ass follow and then split them using train_tset_split and using grid search......
Data_12=[DataSet_1,DataSet_3]
D12=pd.concat(Data_12, axis=0)

#---the best features selected from D12 using orange are 25------

DD12=D12.loc[:,['Testers','Media','lpd_mean','lpd_sum','lpd_thrid_quar','lpd_gmean','rpd_first_quar','lpd_median','lpd_first_quar',
                                          'lpd_max','lpd_hmean','rpd_median','rpd_thrid_quar','rpd_sum','rpd_mean','rpd_gmean','rpd_hmean'
                                          ,'rpd_max','rps_thrid_quar' ,'lps_max'
                                          ,'rps_median','lps_thrid_quar','rps_max','lps_median','rps_mean','lps_mean','rps_hmean']]
                                          #,'rps_gmean','lps_gmean','lps_hmean','rps_first_quar','lps_first_quar']]
                                          #,'rpd_var','rpd_std','lpd_min','rpd_range','lpd_std','lpd_var','lpd_range','rpd_min','lps_min','rps_sum'
                                          #,'lps_sum','rps_min','fpogd_gmean','fpogd_first_quar','total_noFixation']]
                                          #,'fpogd_mean','fpogd_thrid_quar','fpogd_hmean','lps_range','rps_std'
                                          #,'rps_var','fpogd_median','lpd_mad','rps_range','Saccade_scanPath']],'rpd_mad','lpd_iqr','fpogDuration_sum']]

D12.to_csv('Datasets_Pattern/D12.csv', index=False)
D3=DataSet_2
#---the best features selected from D3 using orange are 25------
DD3=D3.loc[:,['Testers','Media','lpd_mean','lpd_sum','lpd_thrid_quar','lpd_gmean','rpd_first_quar','lpd_median','lpd_first_quar',
                                          'lpd_max','lpd_hmean','rpd_median','rpd_thrid_quar','rpd_sum','rpd_mean','rpd_gmean','rpd_hmean'
                                          ,'rpd_max','rps_thrid_quar' ,'lps_max'
                                          ,'rps_median','lps_thrid_quar','rps_max','lps_median','rps_mean','lps_mean','rps_hmean']]
                                          #,'rps_gmean','lps_gmean','lps_hmean','rps_first_quar','lps_first_quar']]
                                          #,'rpd_var','rpd_std','lpd_min','rpd_range','lpd_std','lpd_var','lpd_range','rpd_min','lps_min','rps_sum'
                                          #,'lps_sum','rps_min','fpogd_gmean','fpogd_first_quar','total_noFixation']]
                                          #,'fpogd_mean','fpogd_thrid_quar','fpogd_hmean','lps_range','rps_std'
                                          #,'rps_var','fpogd_median','lpd_mad','rps_range','Saccade_scanPath']],'rpd_mad','lpd_iqr','fpogDuration_sum']]



#..... the Pipeline with the classifiers.......
#..... the SVM model parameters.......
param_grid_svc={'svm__kernel':['rbf','poly','sigmoid','linear'],
            'svm__C':[0.001,0.01,0.1,1,10,100],
            'svm__gamma':[0.001,0.01,0.1,1,10,100]}

list_of_users=['1_User','2_User','3_User','4_User','5_User','6_User','7_User','8_User','9_User','10_User','11_User','12_User','13_User',
               '14_User','15_User','17_User','18_User','19_User','20_User','21_User','22_User','23_User','24_User','25_User','26_User'
               ,'27_User','28_User','29_User','30_User','31_User','32_User']


#-----creating empty dataframes with thier column names----- 
gloAcc_col=['GlobalAccuracy','AverAcc']
df_gloAcc=pd.DataFrame(columns=gloAcc_col)
acc_col=['Testers','Accuracy']
df_acc=pd.DataFrame(columns=acc_col)
perf_indicator=['TP','TN','FP','FN','ERR','FRR','FAR','Sensitivity(TPR)','Specificity(TNR)']
df_perfIndicators=pd.DataFrame(columns=perf_indicator)

#----iterating for each users----
for t in range(0,31):
    #--- Selecting feature vectors of tester T from D12 dataset-------
    SingleTester12=D12.Testers==list_of_users[t] # this array contains the list of users listed above
    SingleTesterData12=D12[SingleTester12]
    #.... assigning label= 1 to the tester T(SingleTesterData12)
    SingleTesterData12.Testers=1
    #--- data set of the rest testers with thier feature vectors  (exculding tester T)     
    RestData12=D12.Testers!=list_of_users[t]
    RestTesterData12=D12[RestData12]   
    #------- Select the feature vectors of tester T from D3-----
    SingleTester3=D3.Testers==list_of_users[t]
    SingleTesterData3=D3[SingleTester3]
    #--- assign label=1 tester T selected from D3
    SingleTesterData3.Testers=1
    #------- data set of the rest testers in D3 with thier feature vectors       
    RestData3=D3.Testers!=list_of_users[t]
    RestTesterData3=D3[RestData3]
    #....repeating the random selection and the prediction for iter_no number of times
    iter_no=20# the number of iterations
    TP=0
    FP=0
    FN=0
    TN=0
    Accuracy=[]
    for i in range(iter_no):   
        #--- randomly select the same number of feature vectors from 'RestTesterData12' which are not cointained in tester T(SingleTesterData12)
        random_subset12 = RestTesterData12.sample(n=20)
        #.... assigning label 0 to the other 20 randomly selected feature vector of the testers
        random_subset12.Testers=0
        #concatinate the randomly selected feature vectors and the testet T feature vectors
        Data12=[SingleTesterData12,random_subset12]
        D=pd.concat(Data12,axis=0)        
        # splitting the new Data set D in to train test sets
        y=D.Testers
        X=D.drop(['Testers','Media'],axis=1)
        X_train,X_test,y_train,y_test=train_test_split(X,y, test_size=0.2)
        #----Feature selection using SelectKbest------
        kbest_svc=SelectKBest(f_classif,k=45)
        kbest_svc.fit(X_train,y_train)
        X_train_kb=kbest_svc.transform(X_train)
        X_test_kb=kbest_svc.transform(X_test)
        pipe_svc_MinMax_kbest= Pipeline([("scaler", preprocessing.MinMaxScaler()), ('kbest_svc', kbest_svc), ("svm", SVC())])
        #---- For all feature considerations-------
        #-----pipe_svc_MinMax= Pipeline([("scaler", preprocessing.MinMaxScaler()), ("svm", SVC())])
        #-----The Grid search with Stratified KFold cross validation kf=5
        grid_search_svc_kb=GridSearchCV(pipe_svc_MinMax_kbest,param_grid_svc,cv=kf)
        grid_search_svc_kb.fit(X_train_kb,y_train)# training the model on the data set of D
        #--- randomly select the same number of feature vectors from 'RestTesterData3' which are not cointained in tester T(SingleTesterData3)
        random_subset3=RestTesterData3.sample(n=10)
        #--- assign label 0 to the class
        random_subset3.Testers=0
        #create the new Data set D1 from random_subset3 + SingleTester3       
        Data3=[SingleTesterData3,random_subset3]
        D1=pd.concat(Data3,axis=0)
        #----separating Dataset D1 which contain feature vector of tester T and other 10 features into label and features
        y_testD1=D1.Testers
        X_testD1=D1.drop(['Testers','Media'],axis=1)
        #--- transform the X_testD1 dataset to K values the feature selector KBest
        X_testD1_kb=kbest_svc.transform(X_testD1)
        #for each feature vector in D1      
        T_tester=SingleTesterData3.loc[:,'Testers']
        D_tester=D1.loc[:,'Testers']
        #----separating tester T into label and features
        y_testT=SingleTesterData3.Testers
        X_testT=SingleTesterData3.drop(['Testers','Media'],axis=1)
        X_testT_kb=kbest_svc.transform(X_testT)
        for v in range(len(D1)):
            #If the feature vector of D1 is the feature vector of tester T
            #(since D1 is the combination of featur vector of T and some other 10 feature vectors selected from D3 which are not combined in feature vector of tester T)
            for s in range(len(SingleTesterData3)):
                if D_tester.values[v]==T_tester.values[s]:
                    if (grid_search_svc_kb.predict(X_testD1_kb)[v])==1:
                        TP = TP + 1
                    else:
                        FN = FN + 1
                else: # other testers not tester T(other testers in D1)                    
                     if (grid_search_svc_kb.predict(X_testD1_kb)[v])==0:
                        TN = TN + 1
                     else:
                        FP = FP + 1 
    #print("tester :",list_of_users[t],"TP:",TP)
    TP=TP/iter_no
    TN=TN/iter_no
    FP=FP/iter_no
    FN=FN/iter_no
    Accuracy=((TP + TN) / (TP + FP + FN + TN))
    err=((FP+FN) / (TP + FP + FN + TN))# ERR Error Rejection Rate
    accuracy={'Testers':list_of_users[t],'Accuracy':Accuracy}
    df_acc=df_acc.append(accuracy,ignore_index=True, verify_integrity=False)
    perfIndicator={'TP':TP,'TN':TN,'FP':FP,'FN':FN,'ERR':err,'FRR':(FN/(FN+TP)),'FAR':FP/(FP+TN),'Sensitivity(TPR)':TP/(TP+FN),'Specificity(TNR)':TN/(TN+FP)}
    df_perfIndicators=df_perfIndicators.append(perfIndicator,ignore_index=True, verify_integrity=False)
globalAccuracy=np.mean(df_acc.loc[:,'Accuracy']) 
print(globalAccuracy)

#------df_gloAcc={'GlobalAccuracy':globalAccuracy} -------  
#------GloAcc=df_acc.append(df_gloAcc,ignore_index=True, verify_integrity=False)------

usersAcc2=[df_acc,df_perfIndicators]
Users_AccuracyVer2=pd.concat(usersAcc2,axis=1)
Users_AccuracyVer2.to_csv('Users_AccuracyVer2')
